{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchmetrics.regression import R2Score\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    RobustScaler,\n",
    "    PolynomialFeatures,\n",
    "    SplineTransformer,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.gaussian_process import kernels, GaussianProcessRegressor\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from plotnine import (\n",
    "    ggplot,\n",
    "    aes,\n",
    "    geom_histogram,\n",
    "    facet_wrap,\n",
    "    geom_point,\n",
    "    geom_smooth,\n",
    "    geom_bar,\n",
    ")\n",
    "\n",
    "torch.set_default_device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self_eval</th>\n",
       "      <th>teacher_eval</th>\n",
       "      <th>extracurricular</th>\n",
       "      <th>district</th>\n",
       "      <th>SRP_1</th>\n",
       "      <th>SRP_2</th>\n",
       "      <th>SRP_3</th>\n",
       "      <th>SRP_4</th>\n",
       "      <th>SRP_5</th>\n",
       "      <th>SRP_6</th>\n",
       "      <th>...</th>\n",
       "      <th>SRP_41</th>\n",
       "      <th>SRP_42</th>\n",
       "      <th>SRP_43</th>\n",
       "      <th>SRP_44</th>\n",
       "      <th>SRP_45</th>\n",
       "      <th>SRP_46</th>\n",
       "      <th>SRP_47</th>\n",
       "      <th>SRP_48</th>\n",
       "      <th>SRP_49</th>\n",
       "      <th>SRP_50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEQN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>969167</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.378</td>\n",
       "      <td>1.581</td>\n",
       "      <td>...</td>\n",
       "      <td>2.070</td>\n",
       "      <td>-1.156</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188942</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>1.603</td>\n",
       "      <td>1.021</td>\n",
       "      <td>0.489</td>\n",
       "      <td>-1.404</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>...</td>\n",
       "      <td>1.478</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>2.021</td>\n",
       "      <td>-1.078</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134058</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.724</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>2.249</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.449</td>\n",
       "      <td>1.980</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>1.592</td>\n",
       "      <td>0.875</td>\n",
       "      <td>-0.734</td>\n",
       "      <td>-2.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124022</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.706</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>1.023</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.249</td>\n",
       "      <td>2.025</td>\n",
       "      <td>-2.289</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.408</td>\n",
       "      <td>1.380</td>\n",
       "      <td>-1.075</td>\n",
       "      <td>-2.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685285</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-1.001</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>0.822</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.661</td>\n",
       "      <td>2.096</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        self_eval  teacher_eval  extracurricular  district  SRP_1  SRP_2  \\\n",
       "SEQN                                                                       \n",
       "969167          4             5                9         3 -0.181 -0.379   \n",
       "188942          4             3                5         4 -0.126  1.603   \n",
       "134058          1             2                8         5  0.724 -0.702   \n",
       "124022          3             3               10         6  0.706 -0.302   \n",
       "685285          5             5                1         5 -0.350 -1.001   \n",
       "\n",
       "        SRP_3  SRP_4  SRP_5  SRP_6  ...  SRP_41  SRP_42  SRP_43  SRP_44  \\\n",
       "SEQN                                ...                                   \n",
       "969167 -0.164  0.080  0.378  1.581  ...   2.070  -1.156  -0.730  -0.508   \n",
       "188942  1.021  0.489 -1.404 -0.955  ...   1.478  -0.318   1.240  -1.993   \n",
       "134058  2.249  0.910  0.330  0.411  ...   0.119   0.449   1.980  -0.401   \n",
       "124022  1.023 -0.895  0.625  1.283  ...   1.249   2.025  -2.289  -0.407   \n",
       "685285  0.931  0.192  0.491  0.292  ...   0.341  -0.118  -0.288   0.457   \n",
       "\n",
       "        SRP_45  SRP_46  SRP_47  SRP_48  SRP_49  SRP_50  \n",
       "SEQN                                                    \n",
       "969167  -0.497   0.224   0.412  -0.517   0.099   0.114  \n",
       "188942   2.021  -1.078  -0.277   0.802   0.253  -0.720  \n",
       "134058  -0.544  -0.944   1.592   0.875  -0.734  -2.336  \n",
       "124022   0.025  -0.515   0.408   1.380  -1.075  -2.451  \n",
       "685285  -0.566   0.822  -0.317   0.661   2.096   0.004  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\", index_col=\"SEQN\")\n",
    "test = pd.read_csv(\"../data/test.csv\", index_col=\"SEQN\")\n",
    "y = train.pop(\"y\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEQN\n",
       "969167   -1.315\n",
       "188942    1.997\n",
       "134058    3.709\n",
       "124022    1.155\n",
       "685285   -1.960\n",
       "          ...  \n",
       "970998   -0.139\n",
       "971286    0.394\n",
       "852862    0.597\n",
       "138992    1.408\n",
       "24075    -2.339\n",
       "Name: y, Length: 8000, dtype: float64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowSum(BaseEstimator):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.values.sum(axis=1).reshape(-1, 1)\n",
    "        else:\n",
    "            return X.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    def fit_transform(self, X, y=None, **kwargs):\n",
    "        return self.transform(X, y, **kwargs)\n",
    "\n",
    "    def get_feature_names_out(self, X, y=None, **kwargs):\n",
    "        return [\"rowSums\"]\n",
    "\n",
    "\n",
    "sumRows = Pipeline(\n",
    "    [\n",
    "        (\"sums\", RowSum()),\n",
    "        (\"scale\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "colTrans = ColumnTransformer(\n",
    "    [\n",
    "        (\"one_hot\", OneHotEncoder(sparse_output=False), [\"district\"]),\n",
    "        (\"collapse\", sumRows, [f\"SRP_{i}\" for i in range(1, 51)]),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "preprocessor = Pipeline(\n",
    "    [\n",
    "        (\"setup\", colTrans),\n",
    "        (\"polys\", PolynomialFeatures(interaction_only=True)),\n",
    "        (\"splines\", SplineTransformer(knots=\"quantile\", n_knots=10, extrapolation=\"continue\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# pd.DataFrame(out, columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.74278879, 0.54474378, 0.87005687, 0.65807509, 1.05137706]), 'score_time': array([0.0091722 , 0.01896024, 0.0129559 , 0.01181793, 0.00823784]), 'test_score': array([ 0.87334896,  0.84110823,  0.8119575 , -0.16400286,  0.88635388])}\n",
      "0.6497531431021331\n"
     ]
    }
   ],
   "source": [
    "model_results = []\n",
    "for model in [\n",
    "    LinearRegression(),\n",
    "    # RandomForestRegressor(n_estimators=256, n_jobs=8)\n",
    "    # HistGradientBoostingRegressor(max_iter=500),\n",
    "]:\n",
    "    model_pipeline = Pipeline([(\"data\", preprocessor), (\"model\", model)])\n",
    "    res = cross_validate(\n",
    "        model_pipeline,\n",
    "        X=train,\n",
    "        y=y,\n",
    "        scoring=make_scorer(r2_score),\n",
    "        return_estimator=False,\n",
    "    )\n",
    "    print(res)\n",
    "    print(res[\"test_score\"].mean())\n",
    "    model_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Pipeline([(\"preprocess\", preprocessor), (\"model\", model)]).fit(train, y=y)\n",
    "test[\"y\"] = final_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colTrans = ColumnTransformer(\n",
    "    [\n",
    "        (\"one_hot\", OneHotEncoder(sparse_output=False), [\"district\"]),\n",
    "        # (\"collapse\", sumRows, [f\"SRP_{i}\" for i in range(1, 51)]),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "nnet_prep = Pipeline(\n",
    "    [\n",
    "        (\"setup\", colTrans),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        # (\"polys\", PolynomialFeatures(interaction_only=True)),\n",
    "        # (\"splines\", SplineTransformer(knots=\"quantile\", n_knots=5, extrapolation=\"linear\")),\n",
    "        (\"tensor\", FunctionTransformer(lambda x: torch.tensor(x, dtype=torch.float32))),\n",
    "    ]\n",
    ")\n",
    "\n",
    "nnet_out = nnet_prep.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepnet = nn.Sequential(\n",
    "#             nn.Linear(nnet_out.shape[1], 16),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(8, 4),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(4, 1),\n",
    "#         )\n",
    "# loss = nn.MSELoss()\n",
    "\n",
    "# mse_loss = R2Score()\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "# print(y_tensor)\n",
    "\n",
    "# tt_split = int(nnet_out.shape[0] * 0.9)\n",
    "\n",
    "class NNetSklearn(BaseEstimator):\n",
    "\n",
    "    def __init__(self, epochs=20, lr=0.01, loss_fn=nn.MSELoss, optimizer=Adam, **optim_args) -> None:\n",
    "        super().__init__()\n",
    "        self.model =  nn.Sequential(\n",
    "            nn.Linear(nnet_out.shape[1], 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 1),\n",
    "        )\n",
    "        self.loss = loss_fn()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer(lr=lr, **optim_args)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.forward(X)\n",
    "\n",
    "    def fit(self, train_x, train_y, val_x, val_y):\n",
    "        for epoch in self.epochs:\n",
    "            preds = self.model(train_x).squeeze()\n",
    "            train_loss = self.loss(train_y, preds)\n",
    "            train_r2 = r2_score(train_y.detach().cpu().numpy(), train_y.detach().cpu().numpy())\n",
    "            train_loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_hat = deepnet(val_x).squeeze()\n",
    "                val_loss = mse_loss(val_y, val_hat)\n",
    "                val_r2 = r2_score(val_y.detach().cpu().numpy(), val_hat.detach().cpu().numpy())\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Train MSE: {train_loss} -- Val MSE: {val_loss} | Train/Val R2: {val_r2} \")\n",
    "\n",
    "\n",
    "\n",
    "# splits\n",
    "\n",
    "# val_x, val_y = nnet_out[tt_split:], y_tensor[tt_split:]\n",
    "# train_x, train_y = nnet_out[:tt_split], y_tensor[:tt_split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Split 1 ---------------------------------------- \n",
      "Epoch 100 -- Train MSE: 0.343 -- Val R2: 0.818\n",
      "Epoch 200 -- Train MSE: 0.204 -- Val R2: 0.827\n",
      "Epoch 300 -- Train MSE: 0.198 -- Val R2: 0.817\n",
      "Epoch 400 -- Train MSE: 0.195 -- Val R2: 0.831\n",
      "Epoch 500 -- Train MSE: 0.186 -- Val R2: 0.811\n",
      "Epoch 600 -- Train MSE: 0.188 -- Val R2: 0.808\n",
      "Epoch 700 -- Train MSE: 0.173 -- Val R2: 0.826\n",
      "Epoch 800 -- Train MSE: 0.175 -- Val R2: 0.813\n",
      "Epoch 900 -- Train MSE: 0.173 -- Val R2: 0.821\n",
      "Epoch 1000 -- Train MSE: 0.179 -- Val R2: 0.808\n",
      "Epoch 1100 -- Train MSE: 0.164 -- Val R2: 0.826\n",
      "Epoch 1200 -- Train MSE: 0.183 -- Val R2: 0.823\n",
      "Epoch 1300 -- Train MSE: 0.161 -- Val R2: 0.822\n",
      "Epoch 1400 -- Train MSE: 0.172 -- Val R2: 0.810\n",
      "Epoch 1500 -- Train MSE: 0.180 -- Val R2: 0.810\n",
      "Epoch 1600 -- Train MSE: 0.160 -- Val R2: 0.825\n",
      "Epoch 1700 -- Train MSE: 0.164 -- Val R2: 0.825\n",
      "Epoch 1800 -- Train MSE: 0.159 -- Val R2: 0.826\n",
      "Epoch 1900 -- Train MSE: 0.169 -- Val R2: 0.824\n",
      "Epoch 2000 -- Train MSE: 0.177 -- Val R2: 0.824\n",
      "Epoch 2100 -- Train MSE: 0.164 -- Val R2: 0.826\n",
      "Epoch 2200 -- Train MSE: 0.157 -- Val R2: 0.828\n",
      "Epoch 2300 -- Train MSE: 0.161 -- Val R2: 0.826\n",
      "Epoch 2400 -- Train MSE: 0.160 -- Val R2: 0.826\n",
      "Epoch 2500 -- Train MSE: 0.156 -- Val R2: 0.827\n",
      "Epoch 2600 -- Train MSE: 0.172 -- Val R2: 0.815\n",
      "Epoch 2700 -- Train MSE: 0.177 -- Val R2: 0.808\n",
      "Epoch 2800 -- Train MSE: 0.177 -- Val R2: 0.805\n",
      "Epoch 2900 -- Train MSE: 0.170 -- Val R2: 0.809\n",
      "Epoch 3000 -- Train MSE: 0.160 -- Val R2: 0.818\n",
      "Epoch 3100 -- Train MSE: 0.157 -- Val R2: 0.822\n",
      "Epoch 3200 -- Train MSE: 0.156 -- Val R2: 0.823\n",
      "Epoch 3300 -- Train MSE: 0.156 -- Val R2: 0.824\n",
      "Epoch 3400 -- Train MSE: 0.164 -- Val R2: 0.827\n",
      "Epoch 3500 -- Train MSE: 0.178 -- Val R2: 0.824\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[226], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     r2_loss \u001b[38;5;241m=\u001b[39m R2Score()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# train_r2 = r2_loss(train_y, y_hat)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     val_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mr2_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# print(f\"Epoch {epoch+1} -- Train MSE: {train_loss.item()/train_x.shape[0]} -- Val MSE: {val_loss.item()/val_x.shape[0]} -- Train R2: {train_r2.item():0.3f} -- Val R2: {val_r2.item():0.3f}\")\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -- Train MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -- Val R2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_r2\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ygtbkm/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ygtbkm/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ygtbkm/lib/python3.12/site-packages/torchmetrics/metric.py:298\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/miniconda3/envs/ygtbkm/lib/python3.12/site-packages/torchmetrics/metric.py:368\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 368\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_count \u001b[38;5;241m=\u001b[39m _update_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ygtbkm/lib/python3.12/site-packages/torchmetrics/metric.py:607\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_context(\n\u001b[1;32m    603\u001b[0m     dist_sync_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_fn,\n\u001b[1;32m    604\u001b[0m     should_sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_sync,\n\u001b[1;32m    605\u001b[0m     should_unsync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_unsync,\n\u001b[1;32m    606\u001b[0m ):\n\u001b[0;32m--> 607\u001b[0m     value \u001b[38;5;241m=\u001b[39m _squeeze_if_scalar(\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_with_cache:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/miniconda3/envs/ygtbkm/lib/python3.12/site-packages/torchmetrics/regression/r2.py:138\u001b[0m, in \u001b[0;36mR2Score.compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute r2 score over the metric states.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_r2_score_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_squared_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjusted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ygtbkm/lib/python3.12/site-packages/torchmetrics/functional/regression/r2.py:90\u001b[0m, in \u001b[0;36m_r2_score_compute\u001b[0;34m(sum_squared_obs, sum_obs, rss, n_obs, adjusted, multioutput)\u001b[0m\n\u001b[1;32m     87\u001b[0m cond \u001b[38;5;241m=\u001b[39m cond_rss \u001b[38;5;241m&\u001b[39m cond_tss\n\u001b[1;32m     89\u001b[0m raw_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(rss)\n\u001b[0;32m---> 90\u001b[0m raw_scores[cond] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[43mrss\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtss\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     91\u001b[0m raw_scores[cond_rss \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mcond_tss] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multioutput \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_values\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ygtbkm/lib/python3.12/site-packages/torch/utils/_device.py:74\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     CURRENT_DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_device\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(exc_type, exc_val, exc_tb)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__torch_function__\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, types, args\u001b[38;5;241m=\u001b[39m(), kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     75\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_mse = 0.0\n",
    "folder = KFold(n_splits=5, shuffle=True)\n",
    "splits = folder.split(nnet_out, y_tensor)\n",
    "for i, (train_id, val_id) in enumerate(splits):\n",
    "    print(f\"---------------------------------------- Split {i+1} ---------------------------------------- \")\n",
    "    train_df, val_df = train.iloc[train_id], train.iloc[val_id]\n",
    "    train_y, val_y = torch.tensor(y.iloc[train_id].values, dtype=torch.float32), torch.tensor(y.iloc[val_id].values, dtype=torch.float32)\n",
    "\n",
    "    train_x = nnet_prep.fit_transform(train_df)\n",
    "    val_x = nnet_prep.transform(val_df)\n",
    "    \n",
    "    deepnet = nn.Sequential(\n",
    "            nn.Linear(nnet_out.shape[1], 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "        )\n",
    "    optimizer = torch.optim.Adam(deepnet.parameters(), lr=0.1, maximize=False)\n",
    "\n",
    "    # train_x, train_y = nnet_out[train_id], y_tensor[train_id]\n",
    "    # val_x, val_y = nnet_out[val_id], y_tensor[val_id]\n",
    "    \n",
    "    # mse_loss = nn.MSELoss()\n",
    "    loss = nn.MSELoss()\n",
    "    deepnet.train()\n",
    "    for epoch in range(5000):\n",
    "        # for x_val, y_val in train_dl:\n",
    "        y_hat = deepnet(train_x).squeeze()\n",
    "        train_loss = loss(train_y, y_hat)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            val_hat = deepnet(val_x).squeeze()\n",
    "            # val_loss = mse_loss(val_y, val_hat)\n",
    "            # train_mse = mse_loss(train_y, y_hat)\n",
    "            r2_loss = R2Score()\n",
    "            # train_r2 = r2_loss(train_y, y_hat)\n",
    "            val_r2 = r2_loss(val_y, val_hat)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            # print(f\"Epoch {epoch+1} -- Train MSE: {train_loss.item()/train_x.shape[0]} -- Val MSE: {val_loss.item()/val_x.shape[0]} -- Train R2: {train_r2.item():0.3f} -- Val R2: {val_r2.item():0.3f}\")\n",
    "            print(f\"Epoch {epoch+1} -- Train MSE: {train_loss.item():0.3f} -- Val R2: {val_r2.item():0.3f}\")\n",
    "            # print(mse / train_x.shape[0], val_loss / val_x.shape[0], val_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500 -- Train R2: -4.476\n",
      "Epoch 1000 -- Train R2: -2.167\n",
      "Epoch 1500 -- Train R2: -1.426\n",
      "Epoch 2000 -- Train R2: -1.053\n",
      "Epoch 2500 -- Train R2: -0.825\n",
      "Epoch 3000 -- Train R2: -0.664\n",
      "Epoch 3500 -- Train R2: -0.546\n",
      "Epoch 4000 -- Train R2: -0.445\n",
      "Epoch 4500 -- Train R2: -0.353\n",
      "Epoch 5000 -- Train R2: -0.257\n",
      "Epoch 5500 -- Train R2: -0.108\n",
      "Epoch 6000 -- Train R2: 0.143\n",
      "Epoch 6500 -- Train R2: 0.296\n",
      "Epoch 7000 -- Train R2: 0.388\n",
      "Epoch 7500 -- Train R2: 0.528\n",
      "Epoch 8000 -- Train R2: 0.651\n",
      "Epoch 8500 -- Train R2: 0.741\n",
      "Epoch 9000 -- Train R2: 0.794\n",
      "Epoch 9500 -- Train R2: 0.825\n",
      "Epoch 10000 -- Train R2: 0.845\n",
      "Epoch 10500 -- Train R2: 0.858\n",
      "Epoch 11000 -- Train R2: 0.868\n",
      "Epoch 11500 -- Train R2: 0.875\n",
      "Epoch 12000 -- Train R2: 0.880\n",
      "Epoch 12500 -- Train R2: 0.883\n",
      "Epoch 13000 -- Train R2: 0.886\n",
      "Epoch 13500 -- Train R2: 0.888\n",
      "Epoch 14000 -- Train R2: 0.890\n",
      "Epoch 14500 -- Train R2: 0.891\n",
      "Epoch 15000 -- Train R2: 0.892\n",
      "Epoch 15500 -- Train R2: 0.893\n",
      "Epoch 16000 -- Train R2: 0.893\n",
      "Epoch 16500 -- Train R2: 0.894\n",
      "Epoch 17000 -- Train R2: 0.893\n",
      "Epoch 17500 -- Train R2: 0.894\n",
      "Epoch 18000 -- Train R2: 0.894\n",
      "Epoch 18500 -- Train R2: 0.894\n",
      "Epoch 19000 -- Train R2: 0.894\n",
      "Epoch 19500 -- Train R2: 0.894\n",
      "Epoch 20000 -- Train R2: 0.895\n",
      "Epoch 20500 -- Train R2: 0.894\n",
      "Epoch 21000 -- Train R2: 0.896\n",
      "Epoch 21500 -- Train R2: 0.895\n",
      "Epoch 22000 -- Train R2: 0.894\n",
      "Epoch 22500 -- Train R2: 0.895\n",
      "Epoch 23000 -- Train R2: 0.897\n",
      "Epoch 23500 -- Train R2: 0.896\n",
      "Epoch 24000 -- Train R2: 0.897\n",
      "Epoch 24500 -- Train R2: 0.896\n",
      "Epoch 25000 -- Train R2: 0.897\n",
      "Epoch 25500 -- Train R2: 0.897\n",
      "Epoch 26000 -- Train R2: 0.895\n",
      "Epoch 26500 -- Train R2: 0.898\n",
      "Epoch 27000 -- Train R2: 0.896\n",
      "Epoch 27500 -- Train R2: 0.897\n",
      "Epoch 28000 -- Train R2: 0.898\n",
      "Epoch 28500 -- Train R2: 0.895\n",
      "Epoch 29000 -- Train R2: 0.898\n",
      "Epoch 29500 -- Train R2: 0.898\n",
      "Epoch 30000 -- Train R2: 0.898\n",
      "Epoch 30500 -- Train R2: 0.898\n",
      "Epoch 31000 -- Train R2: 0.898\n",
      "Epoch 31500 -- Train R2: 0.898\n",
      "Epoch 32000 -- Train R2: 0.899\n",
      "Epoch 32500 -- Train R2: 0.898\n",
      "Epoch 33000 -- Train R2: 0.899\n",
      "Epoch 33500 -- Train R2: 0.899\n",
      "Epoch 34000 -- Train R2: 0.898\n",
      "Epoch 34500 -- Train R2: 0.899\n",
      "Epoch 35000 -- Train R2: 0.899\n",
      "Epoch 35500 -- Train R2: 0.899\n",
      "Epoch 36000 -- Train R2: 0.900\n",
      "Epoch 36500 -- Train R2: 0.900\n",
      "Epoch 37000 -- Train R2: 0.900\n",
      "Epoch 37500 -- Train R2: 0.900\n",
      "Epoch 38000 -- Train R2: 0.898\n",
      "Epoch 38500 -- Train R2: 0.900\n",
      "Epoch 39000 -- Train R2: 0.900\n",
      "Epoch 39500 -- Train R2: 0.900\n",
      "Epoch 40000 -- Train R2: 0.899\n"
     ]
    }
   ],
   "source": [
    "# train_mse = 0.0\n",
    "\n",
    "# Full Training\n",
    "train_x = nnet_prep.fit_transform(train)\n",
    "train_y = y_tensor\n",
    "    \n",
    "deepnet = nn.Sequential(\n",
    "        nn.Linear(nnet_out.shape[1], 24),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(24, 12),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(12, 6),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(6, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(3, 1),\n",
    "    )\n",
    "optimizer = torch.optim.Adam(deepnet.parameters(), lr=0.01, maximize=True)\n",
    "\n",
    "# deepnet.train()\n",
    "loss = R2Score()\n",
    "for epoch in range(40000):\n",
    "    # for x_val, y_val in train_dl:\n",
    "    y_hat = deepnet(train_x).squeeze()\n",
    "    train_loss = loss(train_y, y_hat)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1} -- Train R2: {train_loss.item():0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEQN\n",
       "492834    0.091143\n",
       "309349    1.797333\n",
       "468308    2.242740\n",
       "838812   -0.085628\n",
       "947936    2.875431\n",
       "            ...   \n",
       "971604    2.097824\n",
       "2790      1.671832\n",
       "159210   -0.081907\n",
       "366040   -1.044782\n",
       "901742   -0.484564\n",
       "Name: y, Length: 4000, dtype: float32"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = nnet_prep.transform(test)\n",
    "test[\"y\"] = deepnet.forward(test_out).squeeze().detach().cpu().numpy()\n",
    "test[\"y\"].to_csv(\"./preds.csv\")\n",
    "\n",
    "test[\"y\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ygtbkm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
